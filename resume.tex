\documentclass[7pt]{scrartcl}
\usepackage[a5paper,landscape,margin=5pt]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[small,compact]{titlesec}

\usepackage{hyperref}
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}

\graphicspath{{img/}}

%Commands to format and shorten
\setlength{\columnseprule}{0.2pt}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0em plus 0.2em minus 0.2em}%plus 0.1ex minus 0.2ex}

%% AMSMath operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\E}{E}

%Section numbering
\setcounter{secnumdepth}{1}
\setcounter{section}{1}

\pagestyle{empty} 


\begin{document}
\begin{multicols}{4}
\section{Linear Classification}
\subsection{Perceptron Alg.}
Terminates after no more than $1/\gamma^2$ updates, where 
$\gamma = \min_{i=1,\dots,n} t_i\mathbf{w}_*^T\mathbf{\tilde
\phi}_i$

Normalize features $\phi_i \rightarrow \tilde{\phi_i}$  !

Update rule (when misclassified $t_i\mathbf{w}^{T} \mathbf {\tilde{\phi_i}} \leq 0$): $\mathbf{w} \leftarrow \mathbf{w} +
t_i\mathbf{\tilde{\phi_i}}$

\subsection{Gradient Descent}
Stochastic $\mathbf{w}_{k+1} = \mathbf{w}_k - \eta \nabla_{\mathbf w}E_i(\mathbf{w}_k)$

\paragraph{Least square estimation}
\begin{flalign*} E(\mathbf w) &= \frac12 \sum_{i=1}^n(y(\mathbf x_i)-t_i)^2 \hfill \\ 
&= \frac12  \|\mathbf\Phi\mathbf w- \mathbf t\|^2 
\end{flalign*}

\paragraph{Gradient for Squared Error}
(yields normal equation for least squares!)
\begin{align*} \nabla_{\mathbf w}E&= \sum_{i=1}^n\frac{\partial E}{\partial y_i}
\nabla_{\mathbf w}y_i \\
&= \mathbf\Phi^T( \underbrace{\mathbf \Phi \mathbf w - \mathbf t}_{residual})
\end{align*}


\section{Multi-Layer Perceptron}
\paragraph{Forward pass}
\[a_q^{l} = (\mathbf w_q^{(l)})^T\mathbf z^{(l-1)}+b_q^{(l)}\]
\[z_q^{(l)}=g(a_q^{(l)})\text{, }q=1,..,h_l\]

\paragraph{Backward pass}
\[r^{(L)}=\frac{\partial E_i}{\partial a^{(L)}} = \left\{ \begin{array}{ll}a^L - t_i & for~ E_{sq}  \\ \sigma(a^L) - \tilde{t_i} & for~ E_{log} \end{array} \right. \]
\[r_q^{(l)}=g'(a_q^{(l)})\sum_{j=1}^{h_{l+1}}w_{jq}^{(l+1)}r_j^{(l+1)}\]

\paragraph{Gradient computation}
\[\nabla_{w^{(l)}} E_i = r^{(l+1)} \mathbf{z}^{(l)} ~,~\nabla_{w_q^{(1)}} E_i = r_q^{(1)} \mathbf{x}\]
Wrt b: set $\mathbf{x} = 1$, residual for output layer

\paragraph{Momentum learning}

\[\Delta \mathbf w_k = \mathbf w_{k+1}-\mathbf w_k\]
\[\Delta \mathbf w_k = -\eta(1-\mu)\nabla_{\textbf w_k}E+\mu \Delta \mathbf w_{k-1}\]

$\eta$ = learning rate e.g. $1/k$, $\mu$ = momentum term

\section{Linear Regression. Least Squares Estimation}

\section{Decision Theory}
\paragraph{Risk / Error}
\[ R(f) = \E \left [ L(f(\mathbf x), t) \right ] \]

\paragraph{Bayes-Optimal Classifier}
\begin{align*}
f^*(\mathbf x) &= \argmax_{t \in \tau} P(t|\mathbf x) \\ 
&= \argmax_{t \in \tau} p(\mathbf x|t) P(t) \\
f^*(\mathbf x) &= \argmin_{j \in \tau} \sum_{k \in \tau} L(j,k) \\ 
 &= P(t = k | \mathbf x) \end{align*}

\paragraph{Bayes Error}
\begin{equation*} 1 - \E \left [ \max_{k\in\tau} P(t = k | \mathbf x) \right ] \end{equation*}

\section{Probabilistic Models. Maximum Likelihood}
\paragraph{Covariance}

$$Cov(\mathbf x, \mathbf y) = \E[\mathbf x \mathbf y ^ T] - \E[\mathbf x]E[\mathbf y]$$
$$Cov[\mathbf A \mathbf x] = \mathbf A Cov[\mathbf x] \mathbf A^T$$


\section{Generalization. Regularization}
\section{Conditional Likelihood. Logistic Regression}
\section{Support Vector Machines}
\section{Model Selection and Evaluation}
\section{Dimensionality Reduction}
\section{Unsupervised Learning}
\section{Maths}
Cauchy-Schwarz: $|\mathbf a ^T \mathbf b|\leq \| \mathbf a \| \| \mathbf b \|$
\paragraph{Logistic function}
$$\sigma(v) = \frac{1}{1+e^{-v}}$$
$$\sigma'(v) = \sigma(v)\sigma(-v) = \sigma(v)(1-\sigma(v))$$
\paragraph{tanh}
$$g(a)=\tanh(a)=\frac{e^a-e^{-a}}{e^a + e^{-a}}$$
$$g(a)'=1 - g(a)^2$$
%\appendix{Lagrange Multipliers and Lagrangian Duality}
\end{multicols}
\end{document}
