\documentclass[7pt]{scrartcl}
\usepackage[a5paper,landscape,margin=5pt]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[small,compact]{titlesec}

\usepackage{hyperref}
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}

\graphicspath{{img/}}

%Commands to format and shorten
\setlength{\columnseprule}{0.2pt}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0em plus 0.2em minus 0.2em}%plus 0.1ex minus 0.2ex}

%% AMSMath operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\E}{E}
\renewcommand{\vec}{\mathbf}

%Section numbering
\setcounter{secnumdepth}{1}
\setcounter{section}{1}

\pagestyle{empty} 

\begin{document}
\begin{multicols}{4}
\section{Linear Classification}
\subsection{Perceptron Alg.}
Terminates after no more than $1/\gamma^2$ updates, where 

$\gamma = \min_{i=1,\dots,n} t_i\vec{w}_*^T\vec{\tilde
\phi}_i$

Normalize features $\phi_i \rightarrow \tilde{\phi_i}$  !

Update rule (when misclassified $t_i\vec{w}^{T} \vec {\tilde{\phi_i}} \leq 0$): \[\vec{w} \leftarrow \vec{w} +
t_i\vec{\tilde{\phi_i}}\]

\subsection{Gradient Descent}
Stochastic $\vec{w}_{k+1} = \vec{w}_k - \eta \nabla_{\vec w}E_i(\vec{w}_k)$

\paragraph{Least square estimation}
\begin{flalign*} E(\vec w) &= \frac12 \sum_{i=1}^n(y(\vec x_i)-t_i)^2 \hfill \\ 
&= \frac12  \|\vec\Phi\vec w- \vec t\|^2 
\end{flalign*}

\paragraph{Gradient for Squared Error}
(yields normal equation for least squares if set to 0)
\begin{align*} \nabla_{\vec w}E&= \sum_{i=1}^n\frac{\partial E}{\partial y_i}
\nabla_{\vec w}y_i \\
&= \vec\Phi^T( \underbrace{\vec \Phi \vec w - \vec t}_{residual})
\end{align*}


\section{Multi-Layer Perceptron}
\paragraph{Forward pass}
\[a_q^{l} = (\vec w_q^{(l)})^T\vec z^{(l-1)}+b_q^{(l)}\]
\[z_q^{(l)}=g(a_q^{(l)})\text{, }q=1,..,h_l\]

\paragraph{Backward pass}
\[r^{(L)}=\frac{\partial E_i}{\partial a^{(L)}} = \left\{ \begin{array}{ll}a^L - t_i & for~ E_{sq}  \\ \sigma(a^L) - \tilde{t_i} & for~ E_{log} \end{array} \right. \]
\[r_q^{(l)}=g'(a_q^{(l)})\sum_{j=1}^{h_{l+1}}w_{jq}^{(l+1)}r_j^{(l+1)}\]

\paragraph{Gradient computation}
\[\nabla_{w^{(l)}} E_i = r^{(l+1)} \vec{z}^{(l)} ~,~\nabla_{w_q^{(1)}} E_i = r_q^{(1)} \vec{x}\]
Wrt b: set $\vec{x} = 1$, residual for output layer

\paragraph{Momentum learning}
\[\Delta \vec w_k = \vec w_{k+1}-\vec w_k\]
\[\Delta \vec w_k = -\eta(1-\mu)\nabla_{\textbf w_k}E+\mu \Delta \vec w_{k-1}\]

$\eta$ = learning rate e.g. $1/k$, $\mu$ = momentum term

\section{Linear Regression. Least Squares Estimation}
\paragraph{Univariate Linear Regression}
$\langle x \rangle = n^{-1} \sum_i x_i $. Similar for $t, tx, x^2$
\[y_i = wx_i + b \]
\[\rightarrow w=\frac{Cov(x,t)}{Var(x)}, b=\langle t \rangle - w \langle x \rangle \]

\paragraph{Normal Equations}
\[ \vec{(\Phi}^{T}\vec{\Phi) w = \Phi}^{T} \vec{t}\]
\[ \hat{\vec{w}} = \argmin_w E(\vec{w}) = \vec{(\Phi}^{T}\vec{\Phi)}^{-1} \vec{\Phi}^{T}  \vec{t} \]

\section{Probability. Decision Theory}
\paragraph{Probability}
\[\]
Sum rule: $P(X) = \sum_Y P(X,Y)$\\
Product rule: $P(X,Y) = P(X|Y)P(Y)$
Bayes  \[P(B|F) = \frac{P(F|B)P(B)}{P(F)}\]
\[E[X] = E[E[X|Y]]\]
\[Var(t) = E[Var(t|x)] + Var(E[t|x]) \]
\paragraph{Risk / Error}
\[ R(f) = \E \left [ L(f(\vec x), t) \right ] \]

\paragraph{Bayes-Optimal Classifier}
\begin{align*}
f^*(\vec x) &= \argmax_{t \in \tau} P(t|\vec x) \\ 
&= \argmax_{t \in \tau} p(\vec x|t) P(t) \\
f^*(\vec x) &= \argmin_{j \in \tau} \sum_{k \in \tau} L(j,k) P(t = k | \vec x) \end{align*}

\paragraph{Bayes Error}
\[ R^* = R(f^*) = 1 - \E \left [ \max_{k\in\tau} P(t = k | \vec x) \right ] \]


\paragraph{Discriminant}
\[y^*(x) = log \frac{p(\vec{x}|t=1)}{p(\vec{x}|t=0)} + log \frac{P(t=1)}{P(t=0)} > 0\]
\section{Probabilistic Models. Maximum Likelihood}

\paragraph{Gaussian}
\[N(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
Multivariate $N(\vec{x}|\vec{\mu},\vec{\Sigma}) =$
\[|2\pi\vec{\Sigma}|^{-1/2} e^{-\frac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu})}\]

\paragraph{Correlation}
\[\frac{Cov(x_j,x_k)}{\sqrt{Var(x_j)Var(x_k)}}\]

\paragraph{Covariance}
$$Cov(\vec x, \vec y) = \E[\vec x \vec y ^ T] - \E[\vec x]E[\vec y]$$
$$Cov[\vec A \vec x] = \vec A Cov[\vec x] \vec A^T$$

\paragraph{ML plugin discriminant}
\begin{equation*}
\begin{split}
\hat{y}(\vec{x}) = \hat{\vec{w}}^T \vec{x} - \frac{1}{2}(||\hat{\vec{\mu}}_{+1} ||^2 - \hat{\vec{\mu}}_{-1} ||^2) \\ + log\frac{\hat{\pi}_1}{1-\hat{\pi}_1} ~ where~ \hat{\pi}_1 = n_1 / n ~ and~  \\ \hat{\vec{w}}=\hat{\vec{\mu}}_{+1} - \hat{\vec{\mu}}_{-1}
\end{split}
\end{equation*}
Multiway \[y_k^*(\vec{x}) = -\frac{1}{2}||\vec{x}-\vec{\mu}_k||^2 + log~P(t=k) + C\]

\paragraph{Naive Bayes Classifier}
\[ P(\vec x | N,t=k) = \prod_{m=1}^M \left ( p_m^{(k)} \right
)^{\phi_m(\vec x)} \]

\section{Generalization. Regularization}
\section{Conditional Likelihood. Logistic Regression}
\section{Support Vector Machines}
\section{Model Selection and Evaluation}
\section{Dimensionality Reduction}
\section{Unsupervised Learning}
\section{Maths}
Cauchy-Schwarz: $|\vec a ^T \vec b|\leq \| \vec a \| \| \vec b \|$
\paragraph{Logistic function}
$$\sigma(v) = \frac{1}{1+e^{-v}}$$
$$\sigma'(v) = \sigma(v)\sigma(-v) = \sigma(v)(1-\sigma(v))$$
\paragraph{tanh}
$$g(a)=\tanh(a)=\frac{e^a-e^{-a}}{e^a + e^{-a}}$$
$$g(a)'=1 - g(a)^2$$
%\appendix{Lagrange Multipliers and Lagrangian Duality}
\end{multicols}
\end{document}
