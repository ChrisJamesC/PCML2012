\documentclass[7pt]{scrartcl}
\usepackage[a5paper,landscape,margin=5pt]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[small,compact]{titlesec}

\usepackage{hyperref}
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}

\graphicspath{{img/}}

%Commands to format and shorten
\setlength{\columnseprule}{0.2pt}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0em plus 0.2em minus 0.2em}%plus 0.1ex minus 0.2ex}

%% AMSMath operators
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\E}{E}
\renewcommand{\vec}{\mathbf}

%Section numbering
\setcounter{secnumdepth}{1}
\setcounter{section}{1}

\pagestyle{empty} 
\begin{document}
\begin{multicols}{4}
\section{Linear Classification}
\subsection{Perceptron Alg.}
Terminates after no more than $1/\gamma^2$ updates, where 
$\gamma = \min_{i=1,\dots,n} t_i\vec{w}_*^T\vec{\tilde
\Phi}_i$

Update rule (when misclassified): $\vec{w} \leftarrow \vec{w} +
t_i\vec{\phi_i}$

\subsection{Gradient Descent}
$\vec{w}_{k+1} = \vec{w}_k + \eta E_{\nabla \vec w}(\vec{w}_k)$

\paragraph{Least square estimation}
\begin{flalign*} E(\vec w) &= \frac12 \sum_{i=1}^n(y(\vec x_i)-t_i)^2 \hfill \\ 
&= \frac12  \|\vec\Phi\vec w- \vec t\|^2 
\end{flalign*}

\paragraph{Gradient for Squared Error}
\begin{align*} \nabla_{\vec w}E&= \sum_{i=1}^n\frac{\partial E}{\partial y_i}
\nabla_{\vec w}y_i \\
&= \vec\Phi^T(\vec \Phi \vec w - \vec
t)
\end{align*}


\section{Multi-Layer Perceptron}
\paragraph{Forward pass}
\[a_q^{l} = (\vec w_q^{(l)})^T\vec z^{(l-1)}+b_q^{(l)}\]
\[z_q^{(l)}=g(a_q^{(l)})\text{, }q=1,..,h_l\]

\paragraph{Backward pass}
\[r^{(l)}=\frac{\partial E_i}{\partial a^{(l)}}\]
\[r_q^{(l)}=g'(a_q^{(l)})\sum_{j=1}^{h_{l+1}}w_{jq}^{(l+1)}r_j^{(l+1)}\]

\paragraph{Update rule}

\[\Delta \vec w_k = \vec w_{K+1}-\vec w_k\]
\[\Delta \vec w_k = -\nu(1-\mu)\nabla_{\textbf w_k}E+\mu \Delta \vec w_{k-1}\]

$\nu$ = learning rate, $\mu$ = momentum term

\section{Linear Regression. Least Squares Estimation}

\section{Decision Theory}
\paragraph{Risk / Error}
\[ R(f) = \E \left [ L(f(\vec x), t) \right ] \]

\paragraph{Bayes-Optimal Classifier}
\begin{align*}
f^*(\vec x) &= \argmax_{t \in \tau} P(t|\vec x) \\ 
&= \argmax_{t \in \tau} p(\vec x|t) P(t) \\
f^*(\vec x) &= \argmin_{j \in \tau} \sum_{k \in \tau} L(j,k) \\ 
 &= P(t = k | \vec x) \end{align*}

\paragraph{Bayes Error}
\begin{equation*} 1 - \E \left [ \max_{k\in\tau} P(t = k | \vec x) \right ] \end{equation*}

\section{Probabilistic Models. Maximum Likelihood}
\paragraph{Naive Bayes Classifier}
\[ P(\vec x | N,t=k) = \prod_{m=1}^M \left ( p_m^{(k)} \right
)^{\phi_m(\vec x)} \]
\section{Generalization. Regularization}
\section{Conditional Likelihood. Logistic Regression}
\section{Support Vector Machines}
\section{Model Selection and Evaluation}
\section{Dimensionality Reduction}
\section{Unsupervised Learning}
%\appendix{Lagrange Multipliers and Lagrangian Duality}
\end{multicols}
\end{document}
